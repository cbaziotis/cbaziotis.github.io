---
---


%--------------------------------------------------------------------------
% 2022
%--------------------------------------------------------------------------
@inproceedings{baziotis-2022-hyper-adapters,
    abbr = {EMNLP},
    title = "Multilingual Machine Translation with Hyper-Adapters",
    author = "Baziotis, Christos and Artetxe, Mikel and Cross, James and Bhosale, Shruti",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
    month = Dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    pdf = "https://arxiv.org/abs/2205.10835",
    image = "/assets/img/emnlp-2022-hyper-adapters.jpeg",
    abstract = "Multilingual machine translation suffers from negative interference across languages. A common solution is to relax parameter sharing with language-specific modules like adapters. However, adapters of related languages are unable to transfer information, and their total number of parameters becomes prohibitively expensive as the number of languages grows. In this work, we overcome these drawbacks using hyper-adapters -- hyper-networks that generate adapters from language and layer embeddings. While past work had poor results when scaling hyper-networks, we propose a rescaling fix that significantly improves convergence and enables training larger hyper-networks. We find that hyper-adapters are more parameter efficient than regular adapters, reaching the same performance with up to 12 times less parameters. When using the same number of parameters and FLOPS, our approach consistently outperforms regular adapters. Also, hyper-adapters converge faster than alternative approaches and scale better than regular dense networks. Our analysis shows that hyper-adapters learn to encode language relatedness, enabling positive transfer across languages.",
    blog ="https://twitter.com/cbaziotis/status/1529047913833254912",
    selected ={true},
}


@inproceedings{baziotis-2022-litter,
    abbr = {Preprint},
    title = "Automatic Evaluation and Analysis of Idioms in Neural Machine Translation",
    author = "Baziotis, Christos and Mathur, Prashant and Hasler, Eva}",
    booktitle = "arXiv preprint arXiv:2210.04545",
    year = "2022",
    pdf = "https://arxiv.org/abs/2210.04545",
    image = "/assets/img/arxiv-2022-litter.png",
    abstract ="A major open problem in neural machine translation (NMT) is the translation of idiomatic expressions, such as \"under the weather\". The meaning of these expressions is not composed by the meaning of their constituent words, and NMT models tend to translate them literally (i.e., word-by-word), which leads to confusing and nonsensical translations. Research on idioms in NMT is limited and obstructed by the absence of automatic methods for quantifying these errors. In this work, first, we propose a novel metric for automatically measuring the frequency of literal translation errors without human involvement. Equipped with this metric, we present controlled translation experiments with models trained in different conditions (with/without the test-set idioms) and across a wide range of (global and targeted) metrics and test sets. We explore the role of monolingual pretraining and find that it yields substantial targeted improvements, even without observing any translation examples of the test-set idioms. In our analysis, we probe the role of idiom context. We find that the randomly initialized models are more local or \"myopic\" as they are relatively unaffected by variations of the idiom context, unlike the pretrained ones.",
    blog ="https://twitter.com/cbaziotis/status/1579773941014360064",
    selected ={true},
}


%--------------------------------------------------------------------------
% 2021
%--------------------------------------------------------------------------
@inproceedings{baziotis-2021-pretraining-obectives,
    abbr = {Findings of ACL},
    title = "Exploration of Unsupervised Pretraining Objectives for Machine Translation",
    author = "Baziotis, Christos and Titov, Ivan and Birch, Alexandra and Haddow, Barry",
    booktitle = "Findings of the Association for Computational Linguistics",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pdf = "https://arxiv.org/abs/2106.05634",
    code = "https://github.com/cbaziotis/nmt-pretraining-objectives",
    image = "/assets/img/nmt_pretraining_acl2021.png",
    abstract = "Unsupervised cross-lingual pretraining has achieved strong results in neural machine translation (NMT), by drastically reducing the need for large parallel data. Most approaches adapt masked-language modeling (MLM) to sequence-to-sequence architectures, by masking parts of the input and reconstructing them in the decoder. In this work, we systematically compare masking with alternative objectives that produce inputs resembling real (full) sentences, by reordering and replacing words based on their context. We pretrain models with different methods on English↔German, English↔Nepali and English↔Sinhala monolingual data, and evaluate them on NMT. In (semi-) supervised NMT, varying the pretraining objective leads to surprisingly small differences in the finetuned performance, whereas unsupervised NMT is much more sensitive to it. To understand these results, we thoroughly study the pretrained models using a series of probes and verify that they encode and use information in different ways. We conclude that finetuning on parallel data is mostly sensitive to few properties that are shared by most models, such as a strong decoder, in contrast to unsupervised NMT that also requires models with strong cross-lingual abilities.",
    blog ="https://twitter.com/cbaziotis/status/1404388431590010888",
    selected ={true},
}
%--------------------------------------------------------------------------
% 2020
%--------------------------------------------------------------------------
@inproceedings{baziotis-etal-2020-prior,
abbr = {EMNLP},
title = "Language Model Prior for Low-Resource Neural Machine Translation",
author = "Baziotis, Christos and Haddow, Barry and Birch, Alexandra",
booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
month = nov,
year = "2020",
publisher = "Association for Computational Linguistics",
pdf = "https://www.aclweb.org/anthology/2020.emnlp-main.615/",
slides = "slides_emnlp2020_lm_prior.pdf",
code = "https://github.com/cbaziotis/lm-prior-for-nmt",
image = "/assets/img/lm_prior-9.png",
website = "http://data.statmt.org/cbaziotis/projects/lm-prior/analysis",
abstract = "The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM ``disagrees'' with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.",
selected ={true},
}
%--------------------------------------------------------------------------
% 2019
%--------------------------------------------------------------------------
@inproceedings{baziotis-etal-2019-seq,
abbr = {NAACL},
title = "{SEQ}{^{3}}: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression",
author = "Baziotis, Christos  and
      Androutsopoulos, Ion  and
      Konstas, Ioannis  and
      Potamianos, Alexandros",
booktitle = "Proceedings of the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
month = jun,
year = "2019",
address = "Minneapolis, Minnesota",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/N19-1071",
html = "https://www.aclweb.org/anthology/N19-1071",
pdf = "https://www.aclweb.org/anthology/N19-1071.pdf",
slides = "https://www.aclweb.org/anthology/attachments/N19-1071.Presentation.pdf",
code = "https://github.com/cbaziotis/seq3",
image = "https://github.com/cbaziotis/seq3/raw/master/seq3_architecture.svg",
doi = "10.18653/v1/N19-1071",
pages = "673--681",
abstract = "Neural sequence-to-sequence models are currently the dominant approach in several natural language processing tasks, but require large parallel corpora. We present a sequence-to-sequence-to-sequence autoencoder (SEQ{\^{}}3), consisting of two chained encoder-decoder pairs, with words used as a sequence of discrete latent variables. We apply the proposed model to unsupervised abstractive sentence compression, where the first and last sequences are the input and reconstructed sentences, respectively, while the middle sequence is the compressed sentence. Constraining the length of the latent word sequences forces the model to distill important information from the input. A pretrained language model, acting as a prior over the latent sequences, encourages the compressed sentences to be human-readable. Continuous relaxations enable us to sample from categorical distributions, allowing gradient-based optimization, unlike alternatives that rely on reinforcement learning. The proposed model does not require parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets.",
selected ={true}
}
@inproceedings{margatina-etal-2019-attention,
abbr = {ACL},
title = "Attention-based Conditioning Methods for External Knowledge Integration",
author = "Margatina, Katerina  and
      Baziotis, Christos  and
      Potamianos, Alexandros",
booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
month = jul,
year = "2019",
address = "Florence, Italy",
publisher = "Association for Computational Linguistics",
html = "https://www.aclweb.org/anthology/P19-1385",
pdf = "https://www.aclweb.org/anthology/P19-1385.pdf",
code = "https://github.com/mourga/affective-attention",
image = "https://user-images.githubusercontent.com/28900064/74201901-e0c90d80-4c62-11ea-93ee-c0dfba7b8759.png",
doi = "10.18653/v1/P19-1385",
pages = "3944--3951",
abstract = "In this paper, we present a novel approach for incorporating external knowledge in Recurrent Neural Networks (RNNs). We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures. This form of conditioning on the attention distribution, enforces the contribution of the most salient words for the task at hand. We introduce three methods, namely attentional concatenation, feature-based gating and affine transformation. Experiments on six benchmark datasets show the effectiveness of our methods. Attentional feature-based gating yields consistent performance improvement across tasks. Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture.",
}
@inproceedings{chronopoulou-etal-2019-embarrassingly,
abbr = {NAACL},
title = "An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models",
author = "Chronopoulou, Alexandra  and
      Baziotis, Christos  and
      Potamianos, Alexandros",
booktitle = "Proceedings of the Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
month = jun,
year = "2019",
address = "Minneapolis, Minnesota",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/N19-1213",
html = "https://www.aclweb.org/anthology/N19-1213",
pdf = "https://www.aclweb.org/anthology/N19-1213.pdf",
code = "https://github.com/alexandra-chron/siatl",
image = "https://user-images.githubusercontent.com/30402550/58558299-19d26680-8229-11e9-893d-99d25c911c7a.png",
doi = "10.18653/v1/N19-1213",
pages = "2089--2095",
abstract = "A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.",
}

%--------------------------------------------------------------------------
% 2018
%--------------------------------------------------------------------------
@inproceedings{Tzinis2018,
    abbr = {Interspeech},
    author={Efthymios Tzinis and Georgios Paraskevopoulos and Christos Baziotis and Alexandros Potamianos},
    title={Integrating Recurrence Dynamics for Speech Emotion Recognition},
    year=2018,
    booktitle={Proceedings of Interspeech},
    pages={927--931},
    doi={10.21437/Interspeech.2018-1377},
    url={http://dx.doi.org/10.21437/Interspeech.2018-1377},
    html={http://dx.doi.org/10.21437/Interspeech.2018-1377},
    pdf={https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1377.pdf},
    code={https://github.com/etzinis/nldrp},
    image={/assets/img/RPS_ee.png}
}

@inproceedings{baziotis-etal-2018-ntua,
abbr = {SemEval},
title = "{NTUA}-{SLP} at {S}em{E}val-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive {RNN}s and Transfer Learning",
author = "Baziotis, Christos  and
      Nikolaos, Athanasiou  and
      Chronopoulou, Alexandra  and
      Kolovou, Athanasia  and
      Paraskevopoulos, Georgios  and
      Ellinas, Nikolaos  and
      Narayanan, Shrikanth  and
      Potamianos, Alexandros",
booktitle = "Proceedings of The International Workshop on Semantic Evaluation",
month = jun,
year = "2018",
address = "New Orleans, Louisiana",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/S18-1037",
html = "https://www.aclweb.org/anthology/S18-1037",
pdf = "https://www.aclweb.org/anthology/S18-1037.pdf",
code = "https://github.com/cbaziotis/ntua-slp-semeval2018",
image = "https://d3i71xaburhd42.cloudfront.net/06c9d9473744b2e2c30718084f48512f7a9230b5/1-Figure1-1.png",
award = "1st in Task1E",
doi = "10.18653/v1/S18-1037",
pages = "245--255",
abstract = "In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition: {``}Affect in Tweets{''}. We participated in all subtasks for English tweets. We propose a Bi-LSTM architecture equipped with a multi-layer self attention mechanism. The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable. Our model utilizes a set of word2vec word embeddings trained on a large collection of 550 million Twitter messages, augmented by a set of word affective features. Due to the limited amount of task-specific training data, we opted for a transfer learning approach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A. The proposed approach ranked 1st in Subtask E {``}Multi-Label Emotion Classification{''}, 2nd in Subtask A {``}Emotion Intensity Regression{''} and achieved competitive results in other subtasks.",
}

@inproceedings{baziotis-etal-2018-ntua-slp,
abbr = {SemEval},
title = "{NTUA}-{SLP} at {S}em{E}val-2018 Task 2: Predicting Emojis using {RNN}s with Context-aware Attention",
author = "Baziotis, Christos  and
      Nikolaos, Athanasiou  and
      Kolovou, Athanasia  and
      Paraskevopoulos, Georgios  and
      Ellinas, Nikolaos  and
      Potamianos, Alexandros",
booktitle = "Proceedings of The International Workshop on Semantic Evaluation",
month = jun,
year = "2018",
address = "New Orleans, Louisiana",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/S18-1069",
html = "https://www.aclweb.org/anthology/S18-1069",
pdf = "https://www.aclweb.org/anthology/S18-1069.pdf",
code = "https://github.com/cbaziotis/ntua-slp-semeval2018",
image = "https://d3i71xaburhd42.cloudfront.net/1995f3c0da2ebd9e61f9c95ee1ce6f5fee1dab4b/1-Figure1-1.png",
award = "2nd place",
doi = "10.18653/v1/S18-1069",
pages = "438--444",
abstract = "In this paper we present a deep-learning model that competed at SemEval-2018 Task 2 {``}Multilingual Emoji Prediction{''}. We participated in subtask A, in which we are called to predict the most likely associated emoji in English tweets. The proposed architecture relies on a Long Short-Term Memory network, augmented with an attention mechanism, that conditions the weight of each word, on a {``}context vector{''} which is taken as the aggregation of a tweet{'}s meaning. Moreover, we initialize the embedding layer of our model, with word2vec word embeddings, pretrained on a dataset of 550 million English tweets. Finally, our model does not rely on hand-crafted features or lexicons and is trained end-to-end with back-propagation. We ranked 2nd out of 48 teams.",
}

@inproceedings{baziotis-etal-2018-ntua-slp-semeval,
abbr = {SemEval},
title = "{NTUA}-{SLP} at {S}em{E}val-2018 Task 3: Tracking Ironic Tweets using Ensembles of Word and Character Level Attentive {RNN}s",
author = "Baziotis, Christos  and
      Nikolaos, Athanasiou  and
      Papalampidi, Pinelopi  and
      Kolovou, Athanasia  and
      Paraskevopoulos, Georgios  and
      Ellinas, Nikolaos  and
      Potamianos, Alexandros",
booktitle = "Proceedings of The International Workshop on Semantic Evaluation",
month = jun,
year = "2018",
address = "New Orleans, Louisiana",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/S18-1100",
html = "https://www.aclweb.org/anthology/S18-1100",
pdf = "https://www.aclweb.org/anthology/S18-1100.pdf",
code = "https://github.com/cbaziotis/ntua-slp-semeval2018",
image = "https://d3i71xaburhd42.cloudfront.net/0b5dcd1a232adb4165ef84979404497a7b3e4b2e/1-Figure1-1.png",
doi = "10.18653/v1/S18-1100",
award = "2nd place",
pages = "613--621",
abstract = "In this paper we present two deep-learning systems that competed at SemEval-2018 Task 3 {``}Irony detection in English tweets{''}. We design and ensemble two independent models, based on recurrent neural networks (Bi-LSTM), which operate at the word and character level, in order to capture both the semantic and syntactic information in tweets. Our models are augmented with a self-attention mechanism, in order to identify the most informative words. The embedding layer of our word-level model is initialized with word2vec word embeddings, pretrained on a collection of 550 million English tweets. We did not utilize any handcrafted features, lexicons or external datasets as prior information and our models are trained end-to-end using back propagation on constrained data. Furthermore, we provide visualizations of tweets with annotations for the salient tokens of the attention layer that can help to interpret the inner workings of the proposed models. We ranked 2nd out of 42 teams in Subtask A and 2nd out of 31 teams in Subtask B. However, post-task-completion enhancements of our models achieve state-of-the-art results ranking 1st for both subtasks.",
}
@inproceedings{chronopoulou-etal-2018-ntua,
    abbr = {WASSA-ACL},
    title = "{NTUA}-{SLP} at {IEST} 2018: Ensemble of Neural Transfer Methods for Implicit Emotion Classification",
    author = "Chronopoulou, Alexandra  and
      Margatina, Aikaterini  and
      Baziotis, Christos  and
      Potamianos, Alexandros",
    booktitle = "Proceedings of the Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6209",
    html = "https://www.aclweb.org/anthology/W18-6209",
    pdf = "https://www.aclweb.org/anthology/W18-6209.pdf",
    code = "https://github.com/alexandra-chron/ntua-slp-wassa-iest2018",
    image = "https://d3i71xaburhd42.cloudfront.net/19dbdd97efd2e8622f4a11be564bc495265830d9/2-Figure1-1.png",
    doi = "10.18653/v1/W18-6209",
    pages = "57--64",
    abstract = "In this paper we present our approach to tackle the Implicit Emotion Shared Task (IEST) organized as part of WASSA 2018 at EMNLP 2018. Given a tweet, from which a certain word has been removed, we are asked to predict the emotion of the missing word. In this work, we experiment with neural Transfer Learning (TL) methods. Our models are based on LSTM networks, augmented with a self-attention mechanism. We use the weights of various pretrained models, for initializing specific layers of our networks. We leverage a big collection of unlabeled Twitter messages, for pretraining word2vec word embeddings and a set of diverse language models. Moreover, we utilize a sentiment analysis dataset for pretraining a model, which encodes emotion related information. The submitted model consists of an ensemble of the aforementioned TL models. Our team ranked 3rd out of 30 participants, achieving an F1 score of 0.703.",
}
%--------------------------------------------------------------------------
% 2017
%--------------------------------------------------------------------------
@inproceedings{baziotis-etal-2017-datastories-semeval,
abbr = {SemEval},
title = "{D}ata{S}tories at {S}em{E}val-2017 Task 4: Deep {LSTM} with Attention for Message-level and Topic-based Sentiment Analysis",
author = "Baziotis, Christos  and
      Pelekis, Nikos  and
      Doulkeridis, Christos",
booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
month = aug,
year = "2017",
address = "Vancouver, Canada",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/S17-2126",
html = "https://www.aclweb.org/anthology/S17-2126",
pdf = "https://www.aclweb.org/anthology/S17-2126.pdf",
code = "https://github.com/cbaziotis/datastories-semeval2017-task4",
image = "https://github.com/cbaziotis/datastories-semeval2017-task4/raw/master/task4C-1.png",
award = "1st in Task4A, 2nd in Task4{B,C,D}",
doi = "10.18653/v1/S17-2126",
pages = "747--754",
abstract = "In this paper we present two deep-learning systems that competed at SemEval-2017 Task 4 {``}Sentiment Analysis in Twitter{''}. We participated in all subtasks for English tweets, involving message-level and topic-based sentiment polarity classification and quantification. We use Long Short-Term Memory (LSTM) networks augmented with two kinds of attention mechanisms, on top of word embeddings pre-trained on a big collection of Twitter messages. Also, we present a text processing tool suitable for social network messages, which performs tokenization, word normalization, segmentation and spell correction. Moreover, our approach uses no hand-crafted features or sentiment lexicons. We ranked 1st (tie) in Subtask A, and achieved very competitive results in the rest of the Subtasks. Both the word embeddings and our text processing tool are available to the research community.",
}

@inproceedings{baziotis-etal-2017-datastories,
abbr = {SemEval},
title = "{D}ata{S}tories at {S}em{E}val-2017 Task 6: {S}iamese {LSTM} with Attention for Humorous Text Comparison",
author = "Baziotis, Christos  and
      Pelekis, Nikos  and
      Doulkeridis, Christos",
booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
month = aug,
year = "2017",
address = "Vancouver, Canada",
publisher = "Association for Computational Linguistics",
url = "https://www.aclweb.org/anthology/S17-2065",
html = "https://www.aclweb.org/anthology/S17-2065",
pdf = "https://www.aclweb.org/anthology/S17-2065.pdf",
code = "https://github.com/cbaziotis/datastories-semeval2017-task6",
image = "https://github.com/cbaziotis/datastories-semeval2017-task6/raw/master/task6-1.png",
award = "2nd in Task6A",
doi = "10.18653/v1/S17-2065",
pages = "390--395",
abstract = "In this paper we present a deep-learning system that competed at SemEval-2017 Task 6 ''{\#}HashtagWars: Learning a Sense of Humor{''}. We participated in Subtask A, in which the goal was, given two Twitter messages, to identify which one is funnier. We propose a Siamese architecture with bidirectional Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism. Our system works on the token-level, leveraging word embeddings trained on a big collection of unlabeled Twitter messages. We ranked 2nd in 7 teams. A post-completion improvement of our model, achieves state-of-the-art results on {\#}HashtagWars dataset.",
}