<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Christos  Baziotis</title>
<meta name="description" content="Hi! I am Christos, machine learning researcher.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
<link href="https://emoji-css.afeld.me/emoji.css" rel="stylesheet">
<link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              About
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/assets/pdf/cv_christos_baziotis.pdf">
                CV
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Christos</span>   Baziotis
    </h1>
  </header>

  <p></p>


  <article>
    
    <div class="profile float-left">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/profile.jpg">
      
      
      <div class="contact-list">
        <span class="text-left">
  <a href="mailto:%63%68%72%69%73%74%6F%73.%62%61%7A%69%6F%74%69%73@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a> Email
  
  <br><a href="https://scholar.google.com/citations?user=nP81eYkAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a> Google Scholar
  
  
  <br><a href="https://github.com/cbaziotis" target="_blank" title="GitHub"><i class="fab fa-github"></i></a> GitHub
  <br><a href="https://www.linkedin.com/in/christosbaziotis" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a> LinkedIn
  <br><a href="https://twitter.com/cbaziotis" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a> Twitter
  
  
  
  
</span>


        <div class="contact-note"></div>
      </div>
    </div>
    

    <div class="clearfix" style="text-align: justify">
      <p>I am a machine learning researcher and member of the founding team of <a href="https://samaya.ai/">Samaya AI</a>.
We are developing an AI platform that provides research tools and LLM Agents for finance, serving leading institutions like Morgan Stanley and a number of Wall Street hedge funds.
My research focuses on retrieval (QA, RAG), planning with reinforcement learning, and evaluation of LLM outputs.</p>

<p>Before joining Samaya, I did my PhD at the <a href="http://web.inf.ed.ac.uk/ilcc">ILCC</a> in the University of Edinburgh under the supervision of <a href="http://homepages.inf.ed.ac.uk/bhaddow/">Barry Haddow</a>
and <a href="http://homepages.inf.ed.ac.uk/abmayne/">Alexandra Birch</a>. 
My research focused on multilingual translation, large-scale pretraining, and improving performance in low-resource settings by extracting signal from unlabeled and synthetic data.
During my Ph.D., I did internships as Research Scientist at <a href="https://ai.facebook.com/">Meta AI (FAIR)</a>
in Menlo Park and as Applied Scientist at <a href="https://www.amazon.science/">Amazon AI</a> in Palo Alto.</p>

<p>Before starting my PhD, I was a research assistant working on unsupervised text generation (summarization and translation) at
the <a href="https://www.ntua.gr/en/">National Technical University of Athens</a>
with <a href="http://www2.aueb.gr/users/ion/">Ion Androutsopoulos</a>
and <a href="https://slp.cs.ece.ntua.gr/potam/">Alexandros Potamianos</a>.</p>

    </div>

    <p></p>

    
      <div class="news">
  <h2>News</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Mar 13, 2024</th>
          <td>
            
              Our paper ‚Äú<em>When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale</em>‚Äù has been accepted to NAACL 2024! The camera-ready version is available in https://arxiv.org/abs/2305.14124. See you in Mexico City üá≤üáΩüçªüåµüåÆ!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Nov 1, 2023</th>
          <td>
            
              I successfully defended my Ph.D. thesis and joined Samaya AI as part of the founding team!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 22, 2023</th>
          <td>
            
              Happy to share our new paper
‚ÄúWhen Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale‚Äù!
You can read it on arxiv: <a href="https://arxiv.org/abs/2305.14124">https://arxiv.org/abs/2305.14124</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jan 21, 2023</th>
          <td>
            
              Our paper
‚ÄúAutomatic Evaluation and Analysis of Idioms in Neural Machine Translation‚Äù was accepted to EACL 2023 (Main Track)! ü•≥

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 18, 2022</th>
          <td>
            
              The preprint from my Amazon internship work
‚ÄúAutomatic Evaluation and Analysis of Idioms in Neural Machine Translation‚Äù 
is on Arxiv (<a href="http://arxiv.org/abs/2210.04545">preprint</a>)!

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    <p></p>

    
      <div class="publications">
  <h2>Selected Publications</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">NAACL</abbr>
        
        


        

        
        <a href="/assets/img/arxiv-2023-mmt-ssl.png" target="_blank">
            <img src="/assets/img/arxiv-2023-mmt-ssl.png" width="100%" alt="">
        </a>
        
    </div>

    <div id="baziotis-2024-mmt-ssl" class="col-sm-8">
        
        <div class="title">When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Zhang, Biao,
            
            
            
            
            
            
            
            
            
            Birch, Alexandra,
            
            
            
            
            
            
            
            
            
            and Haddow, Barry
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>
            
            
            2024
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            
            
            <a href="https://arxiv.org/abs/2305.14124" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            <a href="https://twitter.com/cbaziotis/status/1661296521424564226" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Multilingual machine translation (MMT), trained on a mixture of parallel and monolingual data, is key for improving translation in low-resource language pairs. However, the literature offers conflicting results on the performance of different methods. To resolve this, we examine how denoising autoencoding (DAE) and backtranslation (BT) impact MMT under different data conditions and model scales. Unlike prior studies, we use a realistic dataset of 100 directions and consider many domain combinations of monolingual and test data. We find that monolingual data generally helps MMT, but models are surprisingly brittle to domain mismatches, especially at smaller model scales. BT is beneficial when the parallel, monolingual, and test data sources are similar but can be detrimental otherwise, while DAE is less effective than previously reported. Next, we analyze the impact of scale (from 90M to 1.6B parameters) and find it is important for both methods, particularly DAE. As scale increases, DAE transitions from underperforming the parallel-only baseline at 90M to converging with BT performance at 1.6B, and even surpassing it in low-resource. These results offer new insights into how to best use monolingual data in MMT.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">EMNLP</abbr>
        
        


        

        
        <a href="/assets/img/emnlp-2022-hyper-adapters.jpeg" target="_blank">
            <img src="/assets/img/emnlp-2022-hyper-adapters.jpeg" width="100%" alt="">
        </a>
        
    </div>

    <div id="baziotis-2022-hyper-adapters" class="col-sm-8">
        
        <div class="title">Multilingual Machine Translation with Hyper-Adapters</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Artetxe, Mikel,
            
            
            
            
            
            
            
            
            
            Cross, James,
            
            
            
            
            
            
            
            
            
            and Bhosale, Shruti
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference on Empirical Methods in Natural Language Processing</em>
            
            
            2022
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            
            
            <a href="https://arxiv.org/abs/2205.10835" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            <a href="https://twitter.com/cbaziotis/status/1529047913833254912" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Multilingual machine translation suffers from negative interference across languages. A common solution is to relax parameter sharing with language-specific modules like adapters. However, adapters of related languages are unable to transfer information, and their total number of parameters becomes prohibitively expensive as the number of languages grows. In this work, we overcome these drawbacks using hyper-adapters ‚Äì hyper-networks that generate adapters from language and layer embeddings. While past work had poor results when scaling hyper-networks, we propose a rescaling fix that significantly improves convergence and enables training larger hyper-networks. We find that hyper-adapters are more parameter efficient than regular adapters, reaching the same performance with up to 12 times less parameters. When using the same number of parameters and FLOPS, our approach consistently outperforms regular adapters. Also, hyper-adapters converge faster than alternative approaches and scale better than regular dense networks. Our analysis shows that hyper-adapters learn to encode language relatedness, enabling positive transfer across languages.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">Findings of ACL</abbr>
        
        


        

        
        <a href="/assets/img/nmt_pretraining_acl2021.png" target="_blank">
            <img src="/assets/img/nmt_pretraining_acl2021.png" width="100%" alt="">
        </a>
        
    </div>

    <div id="baziotis-2021-pretraining-obectives" class="col-sm-8">
        
        <div class="title">Exploration of Unsupervised Pretraining Objectives for Machine Translation</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Titov, Ivan,
            
            
            
            
            
            
            
            
            
            Birch, Alexandra,
            
            
            
            
            
            
            
            
            
            and Haddow, Barry
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Findings of the Association for Computational Linguistics</em>
            
            
            2021
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            
            
            <a href="https://arxiv.org/abs/2106.05634" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            <a href="https://twitter.com/cbaziotis/status/1404388431590010888" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
            
            
            <a href="https://github.com/cbaziotis/nmt-pretraining-objectives" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Unsupervised cross-lingual pretraining has achieved strong results in neural machine translation (NMT), by drastically reducing the need for large parallel data. Most approaches adapt masked-language modeling (MLM) to sequence-to-sequence architectures, by masking parts of the input and reconstructing them in the decoder. In this work, we systematically compare masking with alternative objectives that produce inputs resembling real (full) sentences, by reordering and replacing words based on their context. We pretrain models with different methods on English‚ÜîGerman, English‚ÜîNepali and English‚ÜîSinhala monolingual data, and evaluate them on NMT. In (semi-) supervised NMT, varying the pretraining objective leads to surprisingly small differences in the finetuned performance, whereas unsupervised NMT is much more sensitive to it. To understand these results, we thoroughly study the pretrained models using a series of probes and verify that they encode and use information in different ways. We conclude that finetuning on parallel data is mostly sensitive to few properties that are shared by most models, such as a strong decoder, in contrast to unsupervised NMT that also requires models with strong cross-lingual abilities.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">EMNLP</abbr>
        
        


        

        
        <a href="/assets/img/lm_prior-9.png" target="_blank">
            <img src="/assets/img/lm_prior-9.png" width="100%" alt="">
        </a>
        
    </div>

    <div id="baziotis-etal-2020-prior" class="col-sm-8">
        
        <div class="title">Language Model Prior for Low-Resource Neural Machine Translation</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Haddow, Barry,
            
            
            
            
            
            
            
            
            
            and Birch, Alexandra
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference on Empirical Methods in Natural Language Processing</em>
            
            
            2020
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            
            
            <a href="https://www.aclweb.org/anthology/2020.emnlp-main.615/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/cbaziotis/lm-prior-for-nmt" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
            <a href="/assets/pdf/slides_emnlp2020_lm_prior.pdf" class="btn btn-sm z-depth-0"
               role="button" target="_blank">Slides</a>
            
            
            
            <a href="http://data.statmt.org/cbaziotis/projects/lm-prior/analysis" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-desktop"></i> Website</a>
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM ‚Äúdisagrees‚Äù with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">NAACL</abbr>
        
        


        

        
        <a href="https://github.com/cbaziotis/seq3/raw/master/seq3_architecture.svg" target="_blank">
            <img src="https://github.com/cbaziotis/seq3/raw/master/seq3_architecture.svg" width="100%" alt="">
        </a>
        
    </div>

    <div id="baziotis-etal-2019-seq" class="col-sm-8">
        
        <div class="title">SEQ^3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Androutsopoulos, Ion,
            
            
            
            
            
            
            
            
            
            Konstas, Ioannis,
            
            
            
            
            
            
            
            
            
            and Potamianos, Alexandros
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>
            
            
            2019
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/N19-1071" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/N19-1071.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/cbaziotis/seq3" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
            <a href="https://www.aclweb.org/anthology/attachments/N19-1071.Presentation.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">
                <i class="far fa-file-powerpoint"></i>
                Slides</a>
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Neural sequence-to-sequence models are currently the dominant approach in several natural language processing tasks, but require large parallel corpora. We present a sequence-to-sequence-to-sequence autoencoder (SEQ\^3), consisting of two chained encoder-decoder pairs, with words used as a sequence of discrete latent variables. We apply the proposed model to unsupervised abstractive sentence compression, where the first and last sequences are the input and reconstructed sentences, respectively, while the middle sequence is the compressed sentence. Constraining the length of the latent word sequences forces the model to distill important information from the input. A pretrained language model, acting as a prior over the latent sequences, encourages the compressed sentences to be human-readable. Continuous relaxations enable us to sample from categorical distributions, allowing gradient-based optimization, unlike alternatives that rely on reinforcement learning. The proposed model does not require parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets.</p>
        </div>
        
    </div>
</div>
</li></ol>
</div>

    

    
  </article>

</div>

    </div>

    <!-- Footer -->

<!--    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2025 Christos  Baziotis.
    
    
  </div>
</footer>

-->

  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  
<!-- Enable Tooltips -->
<script type="text/javascript">
$(function () {$('[data-toggle="tooltip"]').tooltip()})
</script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TKJBEWE8ZL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-TKJBEWE8ZL');
</script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
