<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Christos  Baziotis | Publications</title>
<meta name="description" content="Hi! I am Christos, machine learning researcher.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
<link href="https://emoji-css.afeld.me/emoji.css" rel="stylesheet">
<link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Theming-->

  <script src="/assets/js/theme.js"></script>


    
<!-- MathJax -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Christos</span>   Baziotis
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/assets/pdf/cv_christos_baziotis.pdf">
                CV
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description">The publications are listed in reversed chronological order.</p>
  </header>

  <article>
    <p>For the complete list of publications, check my <a href="https://scholar.google.gr/citations?user=nP81eYkAAAAJ&amp;hl=el&amp;authuser=1">Google Scholar</a> or <a href="https://www.semanticscholar.org/author/Christos-Baziotis/40928701?sort=total-citations">Semantic Scholar</a> profile.</p>

<div class="publications">


  <h2 class="year">2024</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">NAACL</abbr>
        
        


        

        
        <a href="/assets/img/arxiv-2023-mmt-ssl.png" target="_blank">
            <img src="/assets/img/arxiv-2023-mmt-ssl.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="baziotis-2024-mmt-ssl" class="col-sm-8">
        
        <div class="title">When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Zhang, Biao,
            
            
            
            
            
            
            
            
            
            Birch, Alexandra,
            
            
            
            
            
            
            
            
            
            and Haddow, Barry
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>
            
            
            2024
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            
            
            <a href="https://arxiv.org/abs/2305.14124" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            <a href="https://twitter.com/cbaziotis/status/1661296521424564226" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Multilingual machine translation (MMT), trained on a mixture of parallel and monolingual data, is key for improving translation in low-resource language pairs. However, the literature offers conflicting results on the performance of different methods. To resolve this, we examine how denoising autoencoding (DAE) and backtranslation (BT) impact MMT under different data conditions and model scales. Unlike prior studies, we use a realistic dataset of 100 directions and consider many domain combinations of monolingual and test data. We find that monolingual data generally helps MMT, but models are surprisingly brittle to domain mismatches, especially at smaller model scales. BT is beneficial when the parallel, monolingual, and test data sources are similar but can be detrimental otherwise, while DAE is less effective than previously reported. Next, we analyze the impact of scale (from 90M to 1.6B parameters) and find it is important for both methods, particularly DAE. As scale increases, DAE transitions from underperforming the parallel-only baseline at 90M to converging with BT performance at 1.6B, and even surpassing it in low-resource. These results offer new insights into how to best use monolingual data in MMT.</p>
        </div>
        
    </div>
</div>
</li></ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">EACL</abbr>
        
        


        

        
        <a href="/assets/img/arxiv-2022-litter.png" target="_blank">
            <img src="/assets/img/arxiv-2022-litter.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="baziotis-2023-litter" class="col-sm-8">
        
        <div class="title">Automatic Evaluation and Analysis of Idioms in Neural Machine Translation</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Mathur, Prashant,
            
            
            
            
            
            
            
            
            
            and Hasler, Eva
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the European Chapter of the Association for Computational Linguistics</em>
            
            
            2023
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            
            
            <a href="https://arxiv.org/abs/2210.04545" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            <a href="https://twitter.com/cbaziotis/status/1579773941014360064" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>A major open problem in neural machine translation (NMT) is the translation of idiomatic expressions, such as “under the weather”. The meaning of these expressions is not composed by the meaning of their constituent words, and NMT models tend to translate them literally (i.e., word-by-word), which leads to confusing and nonsensical translations. Research on idioms in NMT is limited and obstructed by the absence of automatic methods for quantifying these errors. In this work, first, we propose a novel metric for automatically measuring the frequency of literal translation errors without human involvement. Equipped with this metric, we present controlled translation experiments with models trained in different conditions (with/without the test-set idioms) and across a wide range of (global and targeted) metrics and test sets. We explore the role of monolingual pretraining and find that it yields substantial targeted improvements, even without observing any translation examples of the test-set idioms. In our analysis, we probe the role of idiom context. We find that the randomly initialized models are more local or “myopic” as they are relatively unaffected by variations of the idiom context, unlike the pretrained ones.</p>
        </div>
        
    </div>
</div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">EMNLP</abbr>
        
        


        

        
        <a href="/assets/img/emnlp-2022-hyper-adapters.jpeg" target="_blank">
            <img src="/assets/img/emnlp-2022-hyper-adapters.jpeg" width="100%" alt="" />
        </a>
        
    </div>

    <div id="baziotis-2022-hyper-adapters" class="col-sm-8">
        
        <div class="title">Multilingual Machine Translation with Hyper-Adapters</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Artetxe, Mikel,
            
            
            
            
            
            
            
            
            
            Cross, James,
            
            
            
            
            
            
            
            
            
            and Bhosale, Shruti
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference on Empirical Methods in Natural Language Processing</em>
            
            
            2022
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            
            
            <a href="https://arxiv.org/abs/2205.10835" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            <a href="https://twitter.com/cbaziotis/status/1529047913833254912" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Multilingual machine translation suffers from negative interference across languages. A common solution is to relax parameter sharing with language-specific modules like adapters. However, adapters of related languages are unable to transfer information, and their total number of parameters becomes prohibitively expensive as the number of languages grows. In this work, we overcome these drawbacks using hyper-adapters – hyper-networks that generate adapters from language and layer embeddings. While past work had poor results when scaling hyper-networks, we propose a rescaling fix that significantly improves convergence and enables training larger hyper-networks. We find that hyper-adapters are more parameter efficient than regular adapters, reaching the same performance with up to 12 times less parameters. When using the same number of parameters and FLOPS, our approach consistently outperforms regular adapters. Also, hyper-adapters converge faster than alternative approaches and scale better than regular dense networks. Our analysis shows that hyper-adapters learn to encode language relatedness, enabling positive transfer across languages.</p>
        </div>
        
    </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">Findings of ACL</abbr>
        
        


        

        
        <a href="/assets/img/nmt_pretraining_acl2021.png" target="_blank">
            <img src="/assets/img/nmt_pretraining_acl2021.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="baziotis-2021-pretraining-obectives" class="col-sm-8">
        
        <div class="title">Exploration of Unsupervised Pretraining Objectives for Machine Translation</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Titov, Ivan,
            
            
            
            
            
            
            
            
            
            Birch, Alexandra,
            
            
            
            
            
            
            
            
            
            and Haddow, Barry
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Findings of the Association for Computational Linguistics</em>
            
            
            2021
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            
            
            <a href="https://arxiv.org/abs/2106.05634" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            <a href="https://twitter.com/cbaziotis/status/1404388431590010888" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
            
            
            <a href="https://github.com/cbaziotis/nmt-pretraining-objectives" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Unsupervised cross-lingual pretraining has achieved strong results in neural machine translation (NMT), by drastically reducing the need for large parallel data. Most approaches adapt masked-language modeling (MLM) to sequence-to-sequence architectures, by masking parts of the input and reconstructing them in the decoder. In this work, we systematically compare masking with alternative objectives that produce inputs resembling real (full) sentences, by reordering and replacing words based on their context. We pretrain models with different methods on English↔German, English↔Nepali and English↔Sinhala monolingual data, and evaluate them on NMT. In (semi-) supervised NMT, varying the pretraining objective leads to surprisingly small differences in the finetuned performance, whereas unsupervised NMT is much more sensitive to it. To understand these results, we thoroughly study the pretrained models using a series of probes and verify that they encode and use information in different ways. We conclude that finetuning on parallel data is mostly sensitive to few properties that are shared by most models, such as a strong decoder, in contrast to unsupervised NMT that also requires models with strong cross-lingual abilities.</p>
        </div>
        
    </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">EMNLP</abbr>
        
        


        

        
        <a href="/assets/img/lm_prior-9.png" target="_blank">
            <img src="/assets/img/lm_prior-9.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="baziotis-etal-2020-prior" class="col-sm-8">
        
        <div class="title">Language Model Prior for Low-Resource Neural Machine Translation</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Haddow, Barry,
            
            
            
            
            
            
            
            
            
            and Birch, Alexandra
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference on Empirical Methods in Natural Language Processing</em>
            
            
            2020
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            
            
            <a href="https://www.aclweb.org/anthology/2020.emnlp-main.615/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/cbaziotis/lm-prior-for-nmt" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
            <a href="/assets/pdf/slides_emnlp2020_lm_prior.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
            
            
            
            <a href="http://data.statmt.org/cbaziotis/projects/lm-prior/analysis" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-desktop"></i> Website</a>
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM “disagrees” with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.</p>
        </div>
        
    </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">NAACL</abbr>
        
        


        

        
        <a href="https://github.com/cbaziotis/seq3/raw/master/seq3_architecture.svg" target="_blank">
            <img src="https://github.com/cbaziotis/seq3/raw/master/seq3_architecture.svg" width="100%" alt="" />
        </a>
        
    </div>

    <div id="baziotis-etal-2019-seq" class="col-sm-8">
        
        <div class="title">SEQ^3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Androutsopoulos, Ion,
            
            
            
            
            
            
            
            
            
            Konstas, Ioannis,
            
            
            
            
            
            
            
            
            
            and Potamianos, Alexandros
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>
            
            
            2019
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/N19-1071" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/N19-1071.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/cbaziotis/seq3" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
            <a href="https://www.aclweb.org/anthology/attachments/N19-1071.Presentation.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">
                <i class="far fa-file-powerpoint"></i>
                Slides</a>
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Neural sequence-to-sequence models are currently the dominant approach in several natural language processing tasks, but require large parallel corpora. We present a sequence-to-sequence-to-sequence autoencoder (SEQ\^3), consisting of two chained encoder-decoder pairs, with words used as a sequence of discrete latent variables. We apply the proposed model to unsupervised abstractive sentence compression, where the first and last sequences are the input and reconstructed sentences, respectively, while the middle sequence is the compressed sentence. Constraining the length of the latent word sequences forces the model to distill important information from the input. A pretrained language model, acting as a prior over the latent sequences, encourages the compressed sentences to be human-readable. Continuous relaxations enable us to sample from categorical distributions, allowing gradient-based optimization, unlike alternatives that rely on reinforcement learning. The proposed model does not require parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">ACL</abbr>
        
        


        

        
        <a href="https://user-images.githubusercontent.com/28900064/74201901-e0c90d80-4c62-11ea-93ee-c0dfba7b8759.png" target="_blank">
            <img src="https://user-images.githubusercontent.com/28900064/74201901-e0c90d80-4c62-11ea-93ee-c0dfba7b8759.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="margatina-etal-2019-attention" class="col-sm-8">
        
        <div class="title">Attention-based Conditioning Methods for External Knowledge Integration</div>
        <div class="author">
            
            
            
            
            
            Margatina, Katerina,
            
            
            
            
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            and Potamianos, Alexandros
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Annual Meeting of the Association for Computational Linguistics</em>
            
            
            2019
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/P19-1385" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/P19-1385.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/mourga/affective-attention" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper, we present a novel approach for incorporating external knowledge in Recurrent Neural Networks (RNNs). We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures. This form of conditioning on the attention distribution, enforces the contribution of the most salient words for the task at hand. We introduce three methods, namely attentional concatenation, feature-based gating and affine transformation. Experiments on six benchmark datasets show the effectiveness of our methods. Attentional feature-based gating yields consistent performance improvement across tasks. Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">NAACL</abbr>
        
        


        

        
        <a href="https://user-images.githubusercontent.com/30402550/58558299-19d26680-8229-11e9-893d-99d25c911c7a.png" target="_blank">
            <img src="https://user-images.githubusercontent.com/30402550/58558299-19d26680-8229-11e9-893d-99d25c911c7a.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="chronopoulou-etal-2019-embarrassingly" class="col-sm-8">
        
        <div class="title">An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models</div>
        <div class="author">
            
            
            
            
            
            Chronopoulou, Alexandra,
            
            
            
            
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            and Potamianos, Alexandros
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>
            
            
            2019
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/N19-1213" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/N19-1213.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/alexandra-chron/siatl" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.</p>
        </div>
        
    </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">Interspeech</abbr>
        
        


        

        
        <a href="/assets/img/RPS_ee.png" target="_blank">
            <img src="/assets/img/RPS_ee.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="Tzinis2018" class="col-sm-8">
        
        <div class="title">Integrating Recurrence Dynamics for Speech Emotion Recognition</div>
        <div class="author">
            
            
            
            
            
            Tzinis, Efthymios,
            
            
            
            
            
            
            
            
            
            Paraskevopoulos, Georgios,
            
            
            
            
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            and Potamianos, Alexandros
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of Interspeech</em>
            
            
            2018
            
        </div>
        

        <div class="links">
            
            
            
            <a href="http://dx.doi.org/10.21437/Interspeech.2018-1377" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1377.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/etzinis/nldrp" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">SemEval</abbr>
        
        


        
        <p>
            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.5em;" aria-label="TROPHY"></i>
            <span style="font-size: 0.8em;"> 1st in Task1E</span>
        </p>
        

        
        <a href="https://d3i71xaburhd42.cloudfront.net/06c9d9473744b2e2c30718084f48512f7a9230b5/1-Figure1-1.png" target="_blank">
            <img src="https://d3i71xaburhd42.cloudfront.net/06c9d9473744b2e2c30718084f48512f7a9230b5/1-Figure1-1.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="baziotis-etal-2018-ntua" class="col-sm-8">
        
        <div class="title">NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Nikolaos, Athanasiou,
            
            
            
            
            
            
            
            
            
            Chronopoulou, Alexandra,
            
            
            
            
            
            
            
            
            
            Kolovou, Athanasia,
            
            
            
            
            
            
            
            
            
            Paraskevopoulos, Georgios,
            
            
            
            
            
            
            
            
            
            Ellinas, Nikolaos,
            
            
            
            
            
            
            
            
            
            Narayanan, Shrikanth,
            
            
            
            
            
            
            
            
            
            and Potamianos, Alexandros
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of The International Workshop on Semantic Evaluation</em>
            
            
            2018
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/S18-1037" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/S18-1037.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/cbaziotis/ntua-slp-semeval2018" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper we present deep-learning models that submitted to the SemEval-2018 Task 1 competition: “Affect in Tweets”. We participated in all subtasks for English tweets. We propose a Bi-LSTM architecture equipped with a multi-layer self attention mechanism. The attention mechanism improves the model performance and allows us to identify salient words in tweets, as well as gain insight into the models making them more interpretable. Our model utilizes a set of word2vec word embeddings trained on a large collection of 550 million Twitter messages, augmented by a set of word affective features. Due to the limited amount of task-specific training data, we opted for a transfer learning approach by pretraining the Bi-LSTMs on the dataset of Semeval 2017, Task 4A. The proposed approach ranked 1st in Subtask E “Multi-Label Emotion Classification”, 2nd in Subtask A “Emotion Intensity Regression” and achieved competitive results in other subtasks.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">SemEval</abbr>
        
        


        
        <p>
            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.5em;" aria-label="TROPHY"></i>
            <span style="font-size: 0.8em;"> 2nd place</span>
        </p>
        

        
        <a href="https://d3i71xaburhd42.cloudfront.net/1995f3c0da2ebd9e61f9c95ee1ce6f5fee1dab4b/1-Figure1-1.png" target="_blank">
            <img src="https://d3i71xaburhd42.cloudfront.net/1995f3c0da2ebd9e61f9c95ee1ce6f5fee1dab4b/1-Figure1-1.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="baziotis-etal-2018-ntua-slp" class="col-sm-8">
        
        <div class="title">NTUA-SLP at SemEval-2018 Task 2: Predicting Emojis using RNNs with Context-aware Attention</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Nikolaos, Athanasiou,
            
            
            
            
            
            
            
            
            
            Kolovou, Athanasia,
            
            
            
            
            
            
            
            
            
            Paraskevopoulos, Georgios,
            
            
            
            
            
            
            
            
            
            Ellinas, Nikolaos,
            
            
            
            
            
            
            
            
            
            and Potamianos, Alexandros
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of The International Workshop on Semantic Evaluation</em>
            
            
            2018
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/S18-1069" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/S18-1069.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/cbaziotis/ntua-slp-semeval2018" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper we present a deep-learning model that competed at SemEval-2018 Task 2 “Multilingual Emoji Prediction”. We participated in subtask A, in which we are called to predict the most likely associated emoji in English tweets. The proposed architecture relies on a Long Short-Term Memory network, augmented with an attention mechanism, that conditions the weight of each word, on a “context vector” which is taken as the aggregation of a tweet’s meaning. Moreover, we initialize the embedding layer of our model, with word2vec word embeddings, pretrained on a dataset of 550 million English tweets. Finally, our model does not rely on hand-crafted features or lexicons and is trained end-to-end with back-propagation. We ranked 2nd out of 48 teams.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">SemEval</abbr>
        
        


        
        <p>
            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.5em;" aria-label="TROPHY"></i>
            <span style="font-size: 0.8em;"> 2nd place</span>
        </p>
        

        
        <a href="https://d3i71xaburhd42.cloudfront.net/0b5dcd1a232adb4165ef84979404497a7b3e4b2e/1-Figure1-1.png" target="_blank">
            <img src="https://d3i71xaburhd42.cloudfront.net/0b5dcd1a232adb4165ef84979404497a7b3e4b2e/1-Figure1-1.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="baziotis-etal-2018-ntua-slp-semeval" class="col-sm-8">
        
        <div class="title">NTUA-SLP at SemEval-2018 Task 3: Tracking Ironic Tweets using Ensembles of Word and Character Level Attentive RNNs</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Nikolaos, Athanasiou,
            
            
            
            
            
            
            
            
            
            Papalampidi, Pinelopi,
            
            
            
            
            
            
            
            
            
            Kolovou, Athanasia,
            
            
            
            
            
            
            
            
            
            Paraskevopoulos, Georgios,
            
            
            
            
            
            
            
            
            
            Ellinas, Nikolaos,
            
            
            
            
            
            
            
            
            
            and Potamianos, Alexandros
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of The International Workshop on Semantic Evaluation</em>
            
            
            2018
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/S18-1100" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/S18-1100.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/cbaziotis/ntua-slp-semeval2018" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper we present two deep-learning systems that competed at SemEval-2018 Task 3 “Irony detection in English tweets”. We design and ensemble two independent models, based on recurrent neural networks (Bi-LSTM), which operate at the word and character level, in order to capture both the semantic and syntactic information in tweets. Our models are augmented with a self-attention mechanism, in order to identify the most informative words. The embedding layer of our word-level model is initialized with word2vec word embeddings, pretrained on a collection of 550 million English tweets. We did not utilize any handcrafted features, lexicons or external datasets as prior information and our models are trained end-to-end using back propagation on constrained data. Furthermore, we provide visualizations of tweets with annotations for the salient tokens of the attention layer that can help to interpret the inner workings of the proposed models. We ranked 2nd out of 42 teams in Subtask A and 2nd out of 31 teams in Subtask B. However, post-task-completion enhancements of our models achieve state-of-the-art results ranking 1st for both subtasks.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">WASSA-ACL</abbr>
        
        


        

        
        <a href="https://d3i71xaburhd42.cloudfront.net/19dbdd97efd2e8622f4a11be564bc495265830d9/2-Figure1-1.png" target="_blank">
            <img src="https://d3i71xaburhd42.cloudfront.net/19dbdd97efd2e8622f4a11be564bc495265830d9/2-Figure1-1.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="chronopoulou-etal-2018-ntua" class="col-sm-8">
        
        <div class="title">NTUA-SLP at IEST 2018: Ensemble of Neural Transfer Methods for Implicit Emotion Classification</div>
        <div class="author">
            
            
            
            
            
            Chronopoulou, Alexandra,
            
            
            
            
            
            
            
            
            
            Margatina, Aikaterini,
            
            
            
            
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            and Potamianos, Alexandros
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</em>
            
            
            2018
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/W18-6209" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/W18-6209.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/alexandra-chron/ntua-slp-wassa-iest2018" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper we present our approach to tackle the Implicit Emotion Shared Task (IEST) organized as part of WASSA 2018 at EMNLP 2018. Given a tweet, from which a certain word has been removed, we are asked to predict the emotion of the missing word. In this work, we experiment with neural Transfer Learning (TL) methods. Our models are based on LSTM networks, augmented with a self-attention mechanism. We use the weights of various pretrained models, for initializing specific layers of our networks. We leverage a big collection of unlabeled Twitter messages, for pretraining word2vec word embeddings and a set of diverse language models. Moreover, we utilize a sentiment analysis dataset for pretraining a model, which encodes emotion related information. The submitted model consists of an ensemble of the aforementioned TL models. Our team ranked 3rd out of 30 participants, achieving an F1 score of 0.703.</p>
        </div>
        
    </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">SemEval</abbr>
        
        


        
        <p>
            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.5em;" aria-label="TROPHY"></i>
            <span style="font-size: 0.8em;"> 1st in Task4A, 2nd in Task4B,C,D</span>
        </p>
        

        
        <a href="https://github.com/cbaziotis/datastories-semeval2017-task4/raw/master/task4C-1.png" target="_blank">
            <img src="https://github.com/cbaziotis/datastories-semeval2017-task4/raw/master/task4C-1.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="baziotis-etal-2017-datastories-semeval" class="col-sm-8">
        
        <div class="title">DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for Message-level and Topic-based Sentiment Analysis</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Pelekis, Nikos,
            
            
            
            
            
            
            
            
            
            and Doulkeridis, Christos
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</em>
            
            
            2017
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/S17-2126" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/S17-2126.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/cbaziotis/datastories-semeval2017-task4" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper we present two deep-learning systems that competed at SemEval-2017 Task 4 “Sentiment Analysis in Twitter”. We participated in all subtasks for English tweets, involving message-level and topic-based sentiment polarity classification and quantification. We use Long Short-Term Memory (LSTM) networks augmented with two kinds of attention mechanisms, on top of word embeddings pre-trained on a big collection of Twitter messages. Also, we present a text processing tool suitable for social network messages, which performs tokenization, word normalization, segmentation and spell correction. Moreover, our approach uses no hand-crafted features or sentiment lexicons. We ranked 1st (tie) in Subtask A, and achieved very competitive results in the rest of the Subtasks. Both the word embeddings and our text processing tool are available to the research community.</p>
        </div>
        
    </div>
</div>
</li>
<li><div class="row">
    <div class="col-sm-4 abbr">
        
        
        <abbr class="badge">SemEval</abbr>
        
        


        
        <p>
            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.5em;" aria-label="TROPHY"></i>
            <span style="font-size: 0.8em;"> 2nd in Task6A</span>
        </p>
        

        
        <a href="https://github.com/cbaziotis/datastories-semeval2017-task6/raw/master/task6-1.png" target="_blank">
            <img src="https://github.com/cbaziotis/datastories-semeval2017-task6/raw/master/task6-1.png" width="100%" alt="" />
        </a>
        
    </div>

    <div id="baziotis-etal-2017-datastories" class="col-sm-8">
        
        <div class="title">DataStories at SemEval-2017 Task 6: Siamese LSTM with Attention for Humorous Text Comparison</div>
        <div class="author">
            
            
            
            
            <em>Baziotis, Christos</em>,
            
            
            
            
            
            
            
            
            Pelekis, Nikos,
            
            
            
            
            
            
            
            
            
            and Doulkeridis, Christos
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</em>
            
            
            2017
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/S17-2065" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/S17-2065.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://github.com/cbaziotis/datastories-semeval2017-task6" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper we present a deep-learning system that competed at SemEval-2017 Task 6 ”#HashtagWars: Learning a Sense of Humor”. We participated in Subtask A, in which the goal was, given two Twitter messages, to identify which one is funnier. We propose a Siamese architecture with bidirectional Long Short-Term Memory (LSTM) networks, augmented with an attention mechanism. Our system works on the token-level, leveraging word embeddings trained on a big collection of unlabeled Twitter messages. We ranked 2nd in 7 teams. A post-completion improvement of our model, achieves state-of-the-art results on #HashtagWars dataset.</p>
        </div>
        
    </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

<!--    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2025 Christos  Baziotis.
    
    
  </div>
</footer>

-->

  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  
<!-- Enable Tooltips -->
<script type="text/javascript">
$(function () {$('[data-toggle="tooltip"]').tooltip()})
</script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TKJBEWE8ZL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-TKJBEWE8ZL');
</script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>


</html>
